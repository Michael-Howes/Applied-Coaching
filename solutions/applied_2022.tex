\section{Applied 2022\footnote{Michael Howes}}

\subsection*{Problem 1: R-squared and PCA}

Key ideas:
\begin{itemize}
    \item Connections between principal components analysis and the singular value decomposition.
    \item Orthogonality of principal components and principal component directions.
\end{itemize}

\begin{enumerate}[label=(\alph*)]
    \item We are given that $\bm{X} \in \reals^{n \times p}$  is standardized so that the columns have mean zero and variance one. We are also given that $\bm{X} = \bm{UDV}^\top$ is the SVD of $\bm{X}$. Let $d_1 \ge d_2 \ge \cdots \ge d_p \ge 0$ be the diagonal entries of $\bm{D}$. The columns of $\bm{V}$ are thus the principal component directions and $\frac{1}{n} d_j^2 = \frac{1}{n}\Vert Xv_j \Vert_2^2$ is the variance of $X$ in the $j$th principal component direction. The cumulative percent variance explained sequence is thus,
    \[\rho_k = 100 \times \frac{\sum_{s=1}^k \frac{1}{n}d_s^2}{\sum_{s=1}^p \frac{1}{n}d_s^2}  =100 \times \frac{\sum_{s=1}^k d_s^2}{\sum_{s=1}^p d_s^2} ,\]
    for $k=1,\ldots,p$.
    \item We are given a response $\bm{y} \in \reals^n$ with mean zero and variance one. The fitted values are $\hat{\bm{y}}=\bm{X}\hat{\beta} \in \reals^n$. We know that the fitted values $\hat{\bm{y}}$ are orthogonal to the residuals $\bm{y}-\hat{\bm{y}}$. Thus,
    \[\Vert \bm{y} \Vert_2^2 = \Vert \bm{y}-\hat{\bm{y}} + \hat{\bm{y}} \Vert_2^2 =\Vert \bm{y}-\hat{\bm{y}}\Vert_2^2 + \Vert \hat{\bm{y}} \Vert_2^2.  \]
    And so
    \[\frac{1}{n}\sum_{i=1}^n y_i^2 = \frac{1}{n}\sum_{i=1}^n(y_i-\hat{y}_i)^2 + \frac{1}{n}\sum_{i=1}^n \hat{y}_i^2. \]
    Since $\bm{y}$ has mean zero and variance one, this implies that
    \[1 = \mathrm{MSE}_0 = \mathrm{MSE} + \mathrm{MSS}. \]
    And so,
    \[R^2 = 1-\frac{\mathrm{MSE}}{\mathrm{MSE}_0} = 1-\mathrm{MSE} = \mathrm{MSS}. \]
    \item  We now regress the $j$th column of $\bm{X}$ on the first $k$ principal components of $\bm{X}$. Let $\bm{U}_k = [\bm{u}_1,\ldots,\bm{u}_k] \in \reals^{n \times k}$ be the matrix containing the first $k$ columns of $\bm{U}$. Since $\bm{U}_k^\top \bm{U}_k = \bm{I}_k$, the fitted values from regressing $\bm{x}_j$ on $\bm{U}_k$ are
    \[\hat{\bm{x}}_j = \bm{U}_k(\bm{U}_k^\top \bm{U}_k)^{-1}\bm{U}_k^\top \bm{x}_j =  \bm{U}_k\bm{U}_k^\top \bm{x}_j.\]
    The average regression sum of squares is thus,
    \begin{align*}
        \mathrm{MSS}_j &=\frac{1}{n} \hat{\bm{x}}_j^\top \hat{\bm{x}}_j\\
        &=\frac{1}{n} \bm{x}_j^\top \bm{U}_k\bm{U}_k^\top \bm{U}_k \bm{U}_k^\top \bm{x}_j\\
        &=\frac{1}{n}\bm{x}_j^\top \bm{U}_k\bm{U}_k^\top \bm{x}_j.
    \end{align*}
    Since $\bm{x}_j$ is the $j$th column of $\bm{X}$ we $\bm{x}_j = \bm{X}\bm{e}_j$ where $\bm{e}_j \in \reals^p$ is the $j$th standard basis vector.
    \begin{align*}
        \bm{U}_k^\top \bm{x}_j &= \bm{U}_k\bm{X}\bm{e}_j\\
        &= \bm{U}_k^\top \bm{UDV}^\top \bm{e}_j.
    \end{align*}
    We know that $\bm{U}_k^\top \bm{U} = [\bm{I}_k, \bm{0}_{k \times (p-k)}] \in \reals^{k \times p}$ where $\bm{0}_{k \times (p-k)}$ is a matrix of all zeros of size $k \times (p-k)$. It follows that 
    \begin{align*}
        \bm{U}_k^\top \bm{U}\bm{D}\bm{V}^\top &= [\bm{I}_k, \bm{0}_{k \times (p-k)}]\bm{D}\bm{V}^\top\\
        &=\bm{D}_k\bm{V}_k^\top,
    \end{align*}
    where $\bm{D}_k = \mathrm{diag}(d_1,\ldots, d_k) \in \reals^{k \times k}$ and $\bm{V}_k \in \reals^{p \times k}$ is equal to the first $k$ rows of $\bm{V}$. Thus,
    \begin{align*}
        \bm{U}_k^\top \hat{\bm{x}}_j&=  \bm{D}_k \bm{V}_k^\top \bm{e}_j\\
        &=\sum_{s=1}^k d_s \left(\bm{v}_s^\top \bm{e}_j\right)\bm{e}_s\\
        &=\sum_{s=1}^k d_s v_{sj}\bm{e}_s
    \end{align*}
    where $v_{sj}$ is the entry of $\bm{V}$ in row $s$ and column $j$. We thus have
    \begin{align*}
        \mathrm{MSS}_j&=\frac{1}{n} \Vert \bm{U}_k^\top \hat{\bm{x}}_j \Vert_2^2\\
        &=
        &=\frac{1}{n} \Vert \sum_{s=1}^k d_s v_{sj}\bm{e}_s \Vert_2^2\\
        &=\frac{1}{n}\sum_{s=1}^k d_s^2 v_{sj}^2.
    \end{align*}
    Since $\bm{x}_j$ has mean zero and variance one, we are in the setting of part (b) and hence
    \[R^2_j = \mathrm{MSS}_j = \frac{1}{n} \sum_{s=1}^k d_s^2 v_{sj}^2. \]
    \item Note that
    \begin{align*}
        \sum_{j=1}^p \mathrm{MSS}_j &=\sum_{j=1}^p \frac{1}{n} \sum_{s=1}^k d_s^2 v_{sj}^2\
        &=\sum_{s=1}^k \sum_{j=1}^p \frac{1}{n} d_s^2 v_{sj}^2\\
        &=\sum_{s=1}^k \frac{1}{n} d_s^2 \sum_{j=1}^p v_{sj}^2\\
        &=\sum_{s=1}^k \frac{1}{n}d_s^2 \Vert \bm{v}_s \Vert_2^2 \\
        &=\frac{1}{n}\sum_{s=1}^k d_s^2,
    \end{align*}
    since all rows of $\bm{V}$ have norm one. We know that each column of $\bm{X}$ has variance one and mean zero. Thus,
    \begin{align*}
        p&=\sum_{j=1}^p \frac{1}{n} \bm{x}_j^\top \bm{x}_j\\
        &=\frac{1}{n}\sum_{j=1}^p \tr\left(\bm{x}_j^\top \bm{x}_j\right)\\
        &=\frac{1}{n}\sum_{j=1}^p \tr\left(\bm{x}_j\bm{x}_j^\top \right)\\
        &=\frac{1}{n}\tr\left(\sum_{j=1}^p \bm{x}_j\bm{x}_j^\top \right)\\
        &=\frac{1}{n}\tr\left(\bm{X}\bm{X}^\top\right)\\
        &=\frac{1}{n}\tr\left(\bm{U}\bm{D}\bm{V}^\top \bm{V}\bm{D}\bm{U}^\top\right)\\
        &=\frac{1}{n}\tr\left(\bm{D}^2\right)\\
        &=\frac{1}{n}\sum_{s=1}^p d_s^2.
    \end{align*}
    Thus,
    \begin{align*}
        \frac{100}{p}\sum_{j=1}^p \mathrm{MSS}_j =\frac{100}{np} \sum_{s=1}^k d_s^2=100 \times \frac{\sum_{s=1}^k d_s^2}{\sum_{s=1}^p d_s^2} =\rho_k.
    \end{align*}
    So $\frac{100}{p}\sum_{j=1}^p \mathrm{MSS}_j$ is exactly the cumulative percent variance explained sequence.
\end{enumerate}

\subsection*{Problem 2: LOO CV, PCA and 1NN}

Key ideas:
\begin{itemize}
    \item ``Eyeballing'' principal component directions.
    \item Working out nearest neighbor classifiers.
\end{itemize}
\begin{enumerate}
    \item We are asked to draw a line corresponding to the first principal component. This does not have to be exact, but we can see that a roughly 45 degree line gives the direction with the most variance. Here the principal component direction is represented as a red solid line. In both plots I have also included dashed lines showing the projection onto the first principal direction.
    \begin{figure}[h]
        \begin{center}
            \includegraphics*[width = 0.8\textwidth]{2022-Q2-fig2.png}
        \end{center}
    \end{figure}
    \item Looking at the previous figure we can make two observations. 
    \begin{itemize}
        \item Using the 2D data, the nearest neighbor of a point is one of the adjacent points on the line of points parallel to the PCA direction. For example, for the point at roughly $(-,1,-2)$, the nearest neighbor is the point at roughly $(0,-1.25)$.
        \item Using the projected 1D data from PCA, the nearest neighbor of a point is the point across the PCA direction. This because these points get projected onto the same value when we perform PCA. For example, for the point at roughly $(-1,-2)$, the 1D projected nearest neighbor is the point at roughly $(-2,-1)$.
    \end{itemize}
    To have 100\% error on the 2D data we want an alternating sequence of ``$+$'''s and ``$-$'''s along the two lines of points parallel to the PCA direction. If we use the same sequence of ``$+$'''s and ``$-$'''s on both lines of points, then we will also have 0\% error when using the 1D data. This is because the 1D-nearest neighbor points will have the same labels. 

    To have 0\% error on the 2D data we want adjacent points on the two lines to all have the same label. If we label only line of points with ``$+$'''s and the other with ``$-$'''s, then the 1D data will also have 100\% error. This is because the 1D nearest neighbors will have the opposite labels. 
    
    In summary, the labelling below has the specified error rates.
    \begin{figure}
        [h]
        \begin{center}
            \includegraphics*[width = 0.8\textwidth]{2022-Q2-fig3.png}
        \end{center}
    \end{figure}
\end{enumerate}