\section{Applied 2022: Solution\footnote{Michael Howes}}

\subsection*{Problem 1: R-squared and PCA}

Key ideas:
\begin{itemize}
    \item Connections between principal components analysis and the singular value decomposition.
    \item Orthogonality of principal components and principal component directions.
\end{itemize}

\begin{enumerate}[label=(\alph*)]
    \item We are given that $\mathbf{X} \in \reals^{n \times p}$  is standardized so that the columns have mean zero and variance one. We are also given that $\mathbf{X} = \mathbf{UDV}^\top$ is the SVD of $\mathbf{X}$. Let $d_1 \ge d_2 \ge \cdots \ge d_p \ge 0$ be the diagonal entries of $\mathbf{D}$. The columns of $\mathbf{V}$ are thus the principal component directions and $\frac{1}{n} d_j^2 = \frac{1}{n}\Vert Xv_j \Vert_2^2$ is the variance of $X$ in the $j$th principal component direction. The cumulative percent variance explained sequence is thus,
    \[\rho_k = 100 \times \frac{\sum_{s=1}^k \frac{1}{n}d_s^2}{\sum_{s=1}^p \frac{1}{n}d_s^2}  =100 \times \frac{\sum_{s=1}^k d_s^2}{\sum_{s=1}^p d_s^2} ,\]
    for $k=1,\ldots,p$.
    \item We are given a response $\mathbf{y} \in \reals^n$ with mean zero and variance one. The fitted values are $\hat{\mathbf{y}}=\mathbf{X}\hat{\beta} \in \reals^n$. We know that the fitted values $\hat{\mathbf{y}}$ are orthogonal to the residuals $\mathbf{y}-\hat{\mathbf{y}}$. Thus,
    \[\Vert \mathbf{y} \Vert_2^2 = \Vert \mathbf{y}-\hat{\mathbf{y}} + \hat{\mathbf{y}} \Vert_2^2 =\Vert \mathbf{y}-\hat{\mathbf{y}}\Vert_2^2 + \Vert \hat{\mathbf{y}} \Vert_2^2.  \]
    And so
    \[\frac{1}{n}\sum_{i=1}^n y_i^2 = \frac{1}{n}\sum_{i=1}^n(y_i-\hat{y}_i)^2 + \frac{1}{n}\sum_{i=1}^n \hat{y}_i^2. \]
    Since $\mathbf{y}$ has mean zero and variance one, this implies that
    \[1 = \mathrm{MSE}_0 = \mathrm{MSE} + \mathrm{MSS}. \]
    And so,
    \[R^2 = 1-\frac{\mathrm{MSE}}{\mathrm{MSE}_0} = 1-\mathrm{MSE} = \mathrm{MSS}. \]
    \item  We now regress the $j$th column of $\mathbf{X}$ on the first $k$ principal components of $\mathbf{X}$. Let $\mathbf{U}_k = [\mathbf{u}_1,\ldots,\mathbf{u}_k] \in \reals^{n \times k}$ be the matrix containing the first $k$ columns of $\mathbf{U}$. Since $\mathbf{U}_k^\top \mathbf{U}_k = \mathbf{I}_k$, the fitted values from regressing $\mathbf{x}_j$ on $\mathbf{U}_k$ are
    \[\hat{\mathbf{x}}_j = \mathbf{U}_k(\mathbf{U}_k^\top \mathbf{U}_k)^{-1}\mathbf{U}_k^\top \mathbf{x}_j =  \mathbf{U}_k\mathbf{U}_k^\top \mathbf{x}_j.\]
    The average regression sum of squares is thus,
    \begin{align*}
        \mathrm{MSS}_j &=\frac{1}{n} \hat{\mathbf{x}}_j^\top \hat{\mathbf{x}}_j\\
        &=\frac{1}{n} \mathbf{x}_j^\top \mathbf{U}_k\mathbf{U}_k^\top \mathbf{U}_k \mathbf{U}_k^\top \mathbf{x}_j\\
        &=\frac{1}{n}\mathbf{x}_j^\top \mathbf{U}_k\mathbf{U}_k^\top \mathbf{x}_j.
    \end{align*}
    Since $\mathbf{x}_j$ is the $j$th column of $\mathbf{X}$ we $\mathbf{x}_j = \mathbf{X}\mathbf{e}_j$ where $\mathbf{e}_j \in \reals^p$ is the $j$th standard basis vector.
    \begin{align*}
        \mathbf{U}_k^\top \mathbf{x}_j &= \mathbf{U}_k\mathbf{X}\mathbf{e}_j\\
        &= \mathbf{U}_k^\top \mathbf{UDV}^\top \mathbf{e}_j.
    \end{align*}
    We know that $\mathbf{U}_k^\top \mathbf{U} = [\mathbf{I}_k, \mathbf{0}_{k \times (p-k)}] \in \reals^{k \times p}$ where $\mathbf{0}_{k \times (p-k)}$ is a matrix of all zeros of size $k \times (p-k)$. It follows that 
    \begin{align*}
        \mathbf{U}_k^\top \mathbf{U}\mathbf{D}\mathbf{V}^\top &= [\mathbf{I}_k, \mathbf{0}_{k \times (p-k)}]\mathbf{D}\mathbf{V}^\top\\
        &=\mathbf{D}_k\mathbf{V}_k^\top,
    \end{align*}
    where $\mathbf{D}_k = \mathrm{diag}(d_1,\ldots, d_k) \in \reals^{k \times k}$ and $\mathbf{V}_k \in \reals^{p \times k}$ is equal to the first $k$ rows of $\mathbf{V}$. Thus,
    \begin{align*}
        \mathbf{U}_k^\top \hat{\mathbf{x}}_j&=  \mathbf{D}_k \mathbf{V}_k^\top \mathbf{e}_j\\
        &=\sum_{s=1}^k d_s \left(\mathbf{v}_s^\top \mathbf{e}_j\right)\mathbf{e}_s\\
        &=\sum_{s=1}^k d_s v_{sj}\mathbf{e}_s
    \end{align*}
    where $v_{sj}$ is the entry of $\mathbf{V}$ in row $s$ and column $j$. We thus have
    \begin{align*}
        \mathrm{MSS}_j&=\frac{1}{n} \Vert \mathbf{U}_k^\top \hat{\mathbf{x}}_j \Vert_2^2\\
        &=\frac{1}{n} \left\Vert \sum_{s=1}^k d_s v_{sj}\mathbf{e}_s \right\Vert_2^2\\
        &=\frac{1}{n}\sum_{s=1}^k d_s^2 v_{sj}^2.
    \end{align*}
    Since $\mathbf{x}_j$ has mean zero and variance one, we are in the setting of part (b) and hence
    \[R^2_j = \mathrm{MSS}_j = \frac{1}{n} \sum_{s=1}^k d_s^2 v_{sj}^2. \]
    \item Note that
    \begin{align*}
        \sum_{j=1}^p \mathrm{MSS}_j &=\sum_{j=1}^p \frac{1}{n} \sum_{s=1}^k d_s^2 v_{sj}^2\\
        &=\sum_{s=1}^k \sum_{j=1}^p \frac{1}{n} d_s^2 v_{sj}^2\\
        &=\sum_{s=1}^k \frac{1}{n} d_s^2 \sum_{j=1}^p v_{sj}^2\\
        &=\sum_{s=1}^k \frac{1}{n}d_s^2 \Vert \mathbf{v}_s \Vert_2^2 \\
        &=\frac{1}{n}\sum_{s=1}^k d_s^2,
    \end{align*}
    since all rows of $\mathbf{V}$ have norm one. We know that each column of $\mathbf{X}$ has variance one and mean zero. Thus,
    \begin{align*}
        p&=\sum_{j=1}^p \frac{1}{n} \mathbf{x}_j^\top \mathbf{x}_j\\
        &=\frac{1}{n}\sum_{j=1}^p \tr\left(\mathbf{x}_j^\top \mathbf{x}_j\right)\\
        &=\frac{1}{n}\sum_{j=1}^p \tr\left(\mathbf{x}_j\mathbf{x}_j^\top \right)\\
        &=\frac{1}{n}\tr\left(\sum_{j=1}^p \mathbf{x}_j\mathbf{x}_j^\top \right)\\
        &=\frac{1}{n}\tr\left(\mathbf{X}\mathbf{X}^\top\right)\\
        &=\frac{1}{n}\tr\left(\mathbf{U}\mathbf{D}\mathbf{V}^\top \mathbf{V}\mathbf{D}\mathbf{U}^\top\right)\\
        &=\frac{1}{n}\tr\left(\mathbf{D}^2\right)\\
        &=\frac{1}{n}\sum_{s=1}^p d_s^2.
    \end{align*}
    Thus,
    \begin{align*}
        \frac{100}{p}\sum_{j=1}^p \mathrm{MSS}_j =\frac{100}{np} \sum_{s=1}^k d_s^2=100 \times \frac{\sum_{s=1}^k d_s^2}{\sum_{s=1}^p d_s^2} =\rho_k.
    \end{align*}
    So $\frac{100}{p}\sum_{j=1}^p \mathrm{MSS}_j$ is exactly the cumulative percent variance explained sequence.
\end{enumerate}

\subsection*{Problem 2: LOO, PCA and 1NN}

Key ideas:
\begin{itemize}
    \item ``Eyeballing'' principal component directions.
    \item Working out nearest neighbor classifiers.
\end{itemize}
\begin{enumerate}
    \item We are asked to draw a line corresponding to the first principal component. This does not have to be exact, but we can see that a roughly 45 degree line gives the direction with the most variance. Here the principal component direction is represented as a red solid line. In both plots I have also included dashed lines showing the projection onto the first principal direction.
    \begin{figure}[h]
        \begin{center}
            \includegraphics*[width = 0.8\textwidth]{2022-Q2-fig2.png}
        \end{center}
    \end{figure}
    \item Looking at the previous figure we can make two observations. 
    \begin{itemize}
        \item Using the 2D data, the nearest neighbor of a point is one of the adjacent points on the line of points parallel to the PCA direction. For example, for the point at roughly $(-,1,-2)$, the nearest neighbor is the point at roughly $(0,-1.25)$.
        \item Using the projected 1D data from PCA, the nearest neighbor of a point is the point across the PCA direction. This because these points get projected onto the same value when we perform PCA. For example, for the point at roughly $(-1,-2)$, the 1D projected nearest neighbor is the point at roughly $(-2,-1)$.
    \end{itemize}
    To have 100\% error on the 2D data we want an alternating sequence of ``$+$'''s and ``$-$'''s along the two lines of points parallel to the PCA direction. If we use the same sequence of ``$+$'''s and ``$-$'''s on both lines of points, then we will also have 0\% error when using the 1D data. This is because the 1D-nearest neighbor points will have the same labels. 

    To have 0\% error on the 2D data we want adjacent points on the two lines to all have the same label. If we label only line of points with ``$+$'''s and the other with ``$-$'''s, then the 1D data will also have 100\% error. This is because the 1D nearest neighbors will have the opposite labels. 
    
    In summary, the labelling below has the specified error rates.
    \begin{figure}
        [h]
        \begin{center}
            \includegraphics*[width = 0.8\textwidth]{2022-Q2-fig3.png}
        \end{center}
    \end{figure}
\end{enumerate}

\subsection*{Question 3: Financial Data Science}
Key ideas
\begin{itemize}
    \item General knowledge about supervised learning algorithms.
    \item The dangers of using cross-validation on non i.i.d. data.
\end{itemize}
\begin{enumerate}[label = (\alph*)]
    \item We are asked to describe the strengths of four different supervised learning algorithms for predicting stock returns. These are ridge regression, lasso and elastic net, random forests and gradient boosting.  The goal here should be to just say something for each method. It doesn't have to be very formal or precise. 
    \begin{itemize}
        \item \textbf{Ridge regression} is good at combining many signals across different predictors. Ridge regression can combine weak signals from different predictors to give a strong signal.
        \item \textbf{Lasso and elastic net} is good for feature selection. If we expect that only a few predictors are important for predicting stock returns, then lasso or elastic net would be a good choice since they induce sparsity.
        \item \textbf{Random forests} are non-linear models, and so they are more flexible than the previous two. Random forests are more interpretable and easier to tune than other non-linear models like neural networks.
        \item \textbf{Gradient boosting} is also a non-linear model and hence flexible. Gradient boosting is sequential algorithm, and we can use cross-validation to tune the number of steps used.
    \end{itemize}
    \item We are asked to list any other predictors that would be useful for predicting daily stock returns. Temporal data such as day of the week or month of the year may be helpful. Previous daily stock returns could also be used as a feature. The age of company could also be a useful predictor.
    
    We could also try transforming some of our predictors. For example, we could use log-profits instead of profits or add interactions between sector and company size. It might also be worth-while to transform the response variable. This would be more important if we use a linear model like ridge regression or lasso/elastic net. 
    \item It is not surprising that the test error is much higher than the cross-validation error. This is because our data is temporally correlated. When doing cross-validation we have ignored the temporal dependency. Each data point in the held-out fold will contain points in the other folds that are close in time. On the other hand, the test data contains data from different years to the training data. This means it is much easier to make predictions on the held-out fold than on the test data. As a result, our cross-validation estimate of the test error is biased downwards.

    There is also a second, less severe, problem. The cross-validation procedure was used for model selection not to estimate the test error directly. The estimate of test error is again likely to be downward biased for the test error since we have specifically chosen a model with low CV-error.
    \item We need to use a cross-validation process that respects the temporal dependency. A simple way to do this is by ``blocking'' our data. We could drop data into groups of 1-year or 6-months. Then we should create our CV-folds by choosing groups of data points instead of individual data points. This will ensure that the held-out folds do not contain data points that are close in time to data points in the training folds. This procedure will lead to choosing a different model which hopefully performs better on the training data. 

    The suggestion above does not address the second concern and there may again be some downward bias for selection. In this case, it is not particular important. Since we have a separate test data set, we can just use the test data to estimate the test error from deploying the model. If we want to use all the data from 2010-2019 to train our model and get an estimate of the test error, then we could do nested cross-validation. Again the folds in the nested cross-validation should be based on choosing groups of data points from the same time period not on choosing data points individually.
    \item The out-of-bag error in a random forest is an estimate of test error that is ``almost identical to that obtained by $N$-fold cross-validation'' (Section~15.3 of \citep*{hastie2009elements}). We would therefor expect the out-of-bag error to be higher than the test error for the same reasons discussed in part (c).
\end{enumerate}


\subsection*{Question 4: Infectious Disease Survival Times}
Key ideas:
\begin{itemize}
    \item Testing coefficients in the proportional hazard model.
    \item Testing independence in $2 \times 2 \times K$ tables with the Mantel--Haenszel test.
\end{itemize}
\begin{enumerate}[label=(\alph*)]
    \item Since our data is right censored, we will use tools from survival analysis.  Since we want to control for affects from each region, we can use a proportional hazard model. Suppose we have $N$ subjects and for each subject $i=1,\ldots,N$ we observe 
    \begin{itemize}
        \item An observed time $O_i = \min\{C_i,T_i\}$. The time $T_i$ is the event time (in this case the time at which the subject died after infect). The time $C_i$ is the censoring time which we assume is independent of $T_i$.
        \item A censor-indicator $\delta_i= I_{\{O_i = T_i\}} \in \{0,1\}$ with $\delta_i=0$ meaning subject $i$ is censored.
        \item The vaccination status $V_i \in \{0,1\}$, we'll assume that $V_i=1$ means the subject is vaccinated.
        \item The region $R_i \in \{1,\ldots,J\}$ where $J$ is the number of regions (assumed to be small).
    \end{itemize}
    A proportional hazard models that include effects from both vaccination status and subject regions is
    \begin{equation}\label{eq:2022 Q4 model}T_i\mid V_i,R_i \simind h_i(t), \quad h_i(t) = h_0(t)\exp(\alpha V_i + \beta_{R_i}), \end{equation}
    where we are modelling the distribution of $T_i$ in terms of $T_i$'s hazard function $h_i(t)$. To make this model identifiable, we must add a constraint such as
    \[\sum_{j=1}^J \beta_J = 0 \quad \text{or} \quad \beta_J = 0. \]
    If the number of regions $J$ is not too large, then we will be able to fit the above model by maximizing the partial likelihood. If the number of regions is large, but we have measured features $X_i$ for subjects $i$'s region, then we could use the model
    \[T_i \mid V_i,R_i \simind h_i(t), \quad h_i(t)=h_0(t)\exp(\alpha V_i + \gamma^\top X_i). \]
    Assuming the dimension of $X_i$ is less than $J-1$, then fitting this second model would be easier than fitting \eqref{eq:2022 Q4 model}. We are told that there are only a ``handful'' of possible value of $R$, thus we will assume we have enough data to fit \eqref{eq:2022 Q4 model}. To test the effectiveness of the vaccine, we can test the null hypothesis $\alpha = 0$. If we get evidence that $\alpha <0$, then we can conclude that vaccination likely has a positive effect after controlling for regions. 

    An alternative model would be to include an interaction affect between region and vaccination status. This model would be appropriate if we expect the effectiveness of the vaccine to vary across regions. The corresponding proportional hazard model is,
    \[T_i \mid V_i,R_i \simind h_i(t), \quad h_i(t)=h_0(t)\exp(\alpha_{R_i} V_i + \beta_{R_i}). \]
    Again, testing $\alpha_j=0$ across the regions $j$ will test for the vaccine's effectiveness in region $j$. This probably is not a good choice of model. Firstly, we might not have enough data to fit both $\alpha_j$ and $\beta_j$. Secondly, while it is reasonable to expect health outcomes to vary across regions (represented by $\beta_j$) it is unlikely that the effectiveness of the vaccine will vary across regions (represented by $\alpha_j$). We will thus stick with model \eqref{eq:2022 Q4 model}
    \item In this question, we will ignore the variation across regions. The corresponding Cox proportional hazards model is
    \[T_i \mid V_i,R_i \simind h_i(t), \quad h_i(t)=h_0(t)\exp(\alpha V_i) \]
    This model is fit by maximizing the log partial likelihood,
    \begin{equation}\label{eq:2022 Q4 partial}\ell(\alpha) = \sum_{i:\delta_i=1} \alpha V_i - \log\left(\sum_{k \in R(O_i)} \exp(\alpha V_k)\right), \end{equation}
    where $R(O_i) = \{k : O_k \ge O_i\}$ is the risk-set at time $O_i$. For the null $H_0:\alpha = 0$, the score-test statistic is
    \[T = \frac{\ell'(0)^2}{-\ell''(0)}, \]
    which has asymptotic distribution $\chi^2_1$. We know that the score-test in a Cox proportional hazards model is equivalent to the log-rank test. The log-rank test is itself a special case of the Mantel--Haenszel test. For each $i$ such that $\delta_i=1$, we can make the following contigency table based on the risk set at time $O_i$. Specifically, we consider all individuals with $O_k \ge O_i$ and put them into one of the four below categories
    \begin{table}[h]
        \begin{center}
        \begin{tabular}{|p{2.5cm}|p{5cm}|p{5cm}|}
            \hline
         &$V_k=0$ & $V_k=1$ \\
         \hline 
        $O_k=O_i$ and $\delta_k=1$ & Number of unvaccinated subjects that died at time $O_i$ & Number of vaccinated subjects that died at time $O_i$\\ 
        \hline 
        $O_k > O_i$ or $\delta_k = 0$& Number of unvaccinated subjects that survived beyond time $O_i$ & Number of vaccinated subjects that survived beyond time $O_i$ \\
        \hline 
        \end{tabular}
    \end{center}
    \end{table}
    The null hypothesis that vaccination does not affect survival time implies that the row variable and column variable in the above $2 \times 2$ contingency table are equivalent. If there was just one such table, then we could condition on the row and column sums and use Fisher exact test. However, we actually have $K=|\{i:\delta_i=1\}|$ such tables, and we need a way of combining them. The Mantel--Haenszel test uses the hypergeometric distribution from Fisher's exact test to combine these tables into a test statistic that is asymptotically distributed according to $\chi^2_1$. It turns out that this statistic is exactly the score statistic $T$. This can be proved using the formula for the partial likelihood \eqref{eq:2022 Q4 partial} and the formula for the Mantel--Haenszel test.
    \item Suppose we are now using model \eqref{eq:2022 Q4 model} which allows variation across regions. If we wish to do inference on the affect of vaccination, we can still use the score test. The two hypothesis we are comparing are now,
    \[H_0 : \alpha = 0 \quad \text{vs} \quad H_1 : \alpha \text{ unconstrained}. \]
    Thus, we wish to test if the sub-model which does not include vaccination status is sufficient to explain our data.  To make both the null and alternative models identifiable we will add the constraint $\beta_J=0$ and think of $\beta$ as a vector in $\reals^{J-1}$. The log partial likelihood is equal to 
    \[\ell(\alpha,\beta) = \sum_{i:\delta_i =0}\alpha V_i +\beta_{R_i} - \log\left(\sum_{k \in R(O_i)}\exp\left(\alpha V_k + \beta_{R_k}\right)\right).  \]
    Let 
    \begin{align*}
        U(\alpha,\beta)&=\nabla_{\alpha,\beta}\ell(\alpha,\beta) \in \reals^{J},\\
        I(\alpha,\beta)&=-\nabla_{\alpha,\beta}^2\ell(\alpha,\beta) \in \reals^{J \times J},
    \end{align*}
    be the gradient and negative Hessian of $\ell$ evaluated at $(\alpha,\beta)$. Let $\hat{\beta}_0 \in \reals^{J-1}$ be the MLE of the Cox proportional hazards model under the constraints $\alpha = 0$ and $\beta_J=0$. The score statistic for testing $H_0$ is
    \begin{equation}\label{eq:2022 Q4 score}T = U(0,\hat{\beta}_0)^\top I(0,\hat{\beta}_0)^{-1} U(0,\hat{\beta}_0) \stackrel{\cdot}{\sim} \chi^2_1, \end{equation}
    where the approximation above is asymptotic under the null.  As in part (b), we could again use contingency tables to test $H_0$. Again we have a $2 \times 2$ table for uncensored observation $i$. However, now our table should only include people in the same region of subject $i$. Specifically, if $\delta_i=1$ for some $i$, then we make the following table
    \begin{table}[h]
        \begin{center}
        \begin{tabular}{|p{2.5cm}|p{5cm}|p{5cm}|}
            \hline
         &$V_k=0$ and $R_k=R_i$ & $V_k=1$ and $R_k=R_i$ \\
         \hline 
        $O_k=O_i$ and $\delta_k=1$ & Number of unvaccinated subjects that died at time $O_i$ & Number of vaccinated subjects that died at time $O_i$\\ 
        \hline 
        $O_k > O_i$ or $\delta_k = 0$& Number of unvaccinated subjects that survived beyond time $O_i$ & Number of vaccinated subjects that survived beyond time $O_i$ \\
        \hline 
        \end{tabular}
    \end{center}
    \end{table}
    This is the same as the contingency table in part (b) but now we only included individuals in the same region as subject $i$. Under the null hypothesis $\alpha = 0$, the row and column variables are independent. Thus, conditional on the row and column, the value in the top square is hypergeometric as in Fisher's exact test. We can thus combine these table via the Mantel--Haenszel test and get a test statistic which will be asymptotically $\chi^2_1$ under the null. However, I do not believe that this is the same test statistic as \eqref{eq:2022 Q4 score}.
    
    When adding the region covariates, we have two ways of testing $\alpha = 0$. One is based on the score test and the other is a variant of the Mantel--Haenszel test. While these tests are likely to be related, I believe that they are different. 
\end{enumerate}

\subsection*{Question 5: EM for a mixture of Student-t distributions}

Key ideas
\begin{itemize}
    \item Latent variable models.
    \item The EM algorithm.
\end{itemize}

\begin{enumerate}[label = (\alph*)]
    \item Consider the following augmented model. Independently for $1 \le n \le N$,
    \begin{align*}
        z_n &\sim \mathbf{\pi}, \\
        \tau_n\mid z_n &\sim \mathrm{Ga}\left(\nu_{z_n}/2,\nu_{z_n}/2 \right), \\
        \mathbf{x}_n \mid \tau_n,z_n &\sim \calN\left(\mathbf{\mu}_{z_n}, \tau_n^{-1}\mathbf{\Sigma}_{z_n}\right), 
    \end{align*}
    the joint density for $\{z_n,\tau_n,\mathbf{x}_n\}_{n=1}^N$ corresponding to this model is
    \begin{align*}
        &p\left(\{z_n,\tau_n,\mathbf{x}_n\}_{n=1}^N;\mathbf{\theta}\right)\\
        &=\prod_{n=1}^N p(z_n,\tau_n,\mathbf{x}_n;\mathbf{\theta})\\
        &=\prod_{n=1}^N p(z_n;\mathbf{\theta})p(\tau_n\mid z_n;\mathbf{\theta})p(\mathbf{x}_n\mid \tau_n,z_n;\mathbf{\theta}) \\
        &=\prod_{n=1}^N \pi_{z_n}\mathrm{Ga}\left(\tau_n; \nu_{z_n}/2, \nu_{z_n}/2\right)\calN\left(\mathbf{x}_n; \mathbf{\mu}_{z_n}, \tau_{n}^{-1}\mathbf{\Sigma}_{z_n}\right).
    \end{align*}
    The marginal model for $\{z_n,\mathbf{z}_n\}_{n=1}^N$ is
    \begin{align*}
        &p\left(\{z_n,\mathbf{x}_n\};\mathbf{\theta}\right)\\
        &=\int_{\reals_{> 0}^N} p\left(\{z_n,\tau_n,\mathbf{x}_n\};\mathbf{\theta}\right)d\mathbf{\tau}\\
        &=\int_{\reals_{> 0}^N} \prod_{n=1}^N \pi_{z_n}\mathrm{Ga}\left(\tau_n; \nu_{z_n}/2, \nu_{z_n}/2\right)\calN\left(\mathbf{x}_n; \mathbf{\mu}_{z_n}, \tau_{n}^{-1}\mathbf{\Sigma}_{z_n}\right)d\mathbf{\tau}\\
        &=\prod_{n=1}^N \int  \pi_{z_n}\mathrm{Ga}\left(\tau_n; \nu_{z_n}/2, \nu_{z_n}/2\right)\calN\left(\mathbf{x}_n; \mathbf{\mu}_{z_n}, \tau_{n}^{-1}\mathbf{\Sigma}_{z_n}\right)d\tau_n \\
        &=\prod_{n=1}^N \pi_{z_n}\int  \mathrm{Ga}\left(\tau_n; \nu_{z_n}/2, \nu_{z_n}/2\right)\calN\left(\mathbf{x}_n; \mathbf{\mu}_{z_n}, \tau_{n}^{-1}\mathbf{\Sigma}_{z_n}\right)d\tau_n\\
        &=\prod_{n=1}^N \pi_{z_n}\mathrm{St}\left(\mathbf{x}_n; \nu_{z_n}, \mathbf{\mu}_{z_n}, \tau_{n}^{-1}\mathbf{\Sigma}_{z_n}\right),
    \end{align*}
    where the last line follows from the equation describing the Student-t distribution as a scale mixture of Gaussian. This joint probability corresponds to the original student mixture model.
    \item We are asked to compute,
    \[\omega_{nk} = p(z_n = k \mid \mathbf{x}_n;\mathbf{\theta}), \]
    for $1 \le k \le K$ and $1 \le n \le N$. By Bayes rule,
    \begin{align*}
        \omega_{nk} &= \frac{p(\mathbf{x}_n \mid z_n = k; \mathbf{\theta})p(z_n = k ;\mathbf{\theta})}{\sum_{j=1}^K p(\mathbf{x}_n \mid z_n = j; \mathbf{\theta})p(z_n = j ;\mathbf{\theta})}\\
        &= \frac{\mathrm{St}(\mathbf{x}_n ; \nu_k,\mathbf{\mu}_k, \mathbf{\Sigma}_k)\pi_k}{\sum_{j=1}^K \mathrm{St}(\mathbf{x}_n ; \nu_j,\mathbf{\mu}_j, \mathbf{\Sigma}_j)\pi_j},
    \end{align*}
    since $\mathbf{x}_n \mid z_n \sim \mathrm{St}\left(\nu_{z_n},\mathbf{\mu}_{z_n},\mathbf{\Sigma}_{z_n}\right)$.
    \item Next we are asked to compute the conditional distribution,
    \[p(\tau_n \mid z_n=k, \mathbf{x}_n; \mathbf{\theta}). \]
    To do this, we will again use Bayes rule conditional on $z_n=k$,
    \begin{align*}
        &p(\tau_n \mid z_n=k, \mathbf{x}_n; \mathbf{\theta})\\
        &\propto p(\tau_n \mid z_n=k;\mathbf{\theta})p(\mathbf{x}_n \mid \tau_n,z_n=k;\mathbf{\theta})\\
        &= \mathrm{Ga}(\tau_n;\nu_k/2, \nu_k/2)\calN(\mathbf{x}_n; \mathbf{\mu}_k, \tau_n^{-1}\mathbf{\Sigma}_k)\\
        &\propto \tau_n^{\nu_k/2 - 1}\exp\left(-\nu_k\tau/2\right)\det(\tau_n^{-1}\mathbf{\Sigma}_k)^{-1/2}\exp\left(-\frac{1}{2}(\mathbf{x}_n-\mathbf{\mu}_k)^\top\left(\tau_n^{-1}\mathbf{\Sigma}_k\right)^{-1}(\mathbf{x}_n - \mathbf{\mu}_k)\right).
    \end{align*}
    Note that,
    \[\det(\tau_n^{-1}\mathbf{\Sigma}_k) = \det(\tau_n^{-1}\mathbf{I}_D\mathbf{\Sigma}_k^{-1}) = \tau_n^{-D}\det(\mathbf{\Sigma}_k^{-1}), \]
    and
    \[-\frac{1}{2}(\mathbf{x}_n-\mathbf{\mu}_k)^\top\left(\tau_n^{-1}\mathbf{\Sigma}_k\right)^{-1}(\mathbf{x}_n - \mathbf{\mu}_k) =  - \frac{1}{2}(\mathbf{x}_n-\mathbf{\mu}_k)^\top\mathbf{\Sigma}_k^{-1}(\mathbf{x}_n - \mathbf{\mu}_k)\tau_n.\]
    Thus,
    \begin{align*}
        &p(\tau_n\mid z_n=k \mathbf{x}_n;\mathbf{\theta})\\
        &\propto \tau_n^{\nu_k/2 - 1}\exp\left(-\nu_k\tau/2\right)\tau^{D/2}\det(\mathbf{\Sigma}_k)^{-1/2}\exp\left(\frac{1}{2}(\mathbf{x}_n-\mathbf{\mu}_k)^\top\mathbf{\Sigma}_k^{-1}(\mathbf{x}_n - \mathbf{\mu}_k)\tau_n\right)\\
        &=\tau_n^{(\nu_k+D)/2 - 1}\exp\left(-\frac{1}{2}\left(\nu_k + (\mathbf{x}_n-\mathbf{\mu}_k)^\top\mathbf{\Sigma}_k^{-1}(\mathbf{x}_n - \mathbf{\mu}_k)\right)\tau_n\right)\\
        &\propto \mathrm{Ga}(\tau_n; \alpha_{nk},\beta_{nk}),
    \end{align*}
    where
    \begin{align*}
        \alpha_{nk}&=\frac{\nu_k+D}{2}\\
        \beta_{nk}&=\frac{1}{2}\left(\nu_k + (\mathbf{x}_n-\mathbf{\mu}_k)^\top\mathbf{\Sigma}_k^{-1}(\mathbf{x}_n - \mathbf{\mu}_k)\right).
    \end{align*}
    \item We will first simplify the expected complete log-likelihood,
    \[\mathcal{L}(\mathbf{\theta};\mathbf{\theta}_{\mathrm{old}}) = \sum_{n=1}^N \bbE_{p(z_n, \tau_n \mid \mathbf{x}_n;\mathbf{\theta}_{\mathrm{old}})}\left[\log p(z_n,\tau_n,\mathbf{x}_n;\mathbf{\theta})\right]. \]
    First recall that,
    \begin{align*}
        &\log p(z_n,\tau_n,\mathbf{z}_n;\mathbf{\theta})\\
        &=\sum_{k=1}^K I[z_n = k]\log p(z_n = k,\tau_n,\mathbf{z}_n;\mathbf{\theta})\\
        &=\sum_{k=1}^K I[z_n = k]\left(\log \pi_k + \log \mathrm{Ga}(\tau_n;\nu_k/2,\nu_k/2) + \log \calN(\mathbf{x}_n; \mathbf{\mu}_k, \tau_n^{-1}\mathbf{\Sigma}_k)\right)\\
        &=\sum_{k=1}^K I[z_n = k]\bigg(\log \pi_k + \frac{\nu_k}{2}\log\left(\nu_k/2\right) - \log \Gamma(\nu_k/2) + (\nu_k/2-1)\log(\tau_n)\\
        & - \frac{\nu_k}{2}\tau_n- \frac{D}{2}\log(2\pi) -\frac{1}{2}\log \det(\tau_n^{-1}\mathbf{\Sigma}_k) - \frac{\tau_n}{2}(\mathbf{x}_n - \mathbf{\mu}_k)^\top \mathbf{\Sigma}_k^{-1}(\mathbf{x}_n - \mathbf{\mu}_k) \bigg).
    \end{align*}
    If we drop terms that don't depend on $\mathbf{\theta}$, we have
    \begin{align}
        &\log p(z_n,\tau_n,\mathbf{z}_n;\mathbf{\theta})\nonumber \\
        &=\sum_{k=1}^K I[z_n = k]\bigg(\log \pi_k + \frac{\nu_k}{2}\log\left(\nu_k/2\right) - \log \Gamma(\nu_k/2) + (\nu_k/2-1)\log(\tau_n)\nonumber \\
        & - \frac{\nu_k}{2}\tau_n -\frac{1}{2}\log \det(\tau_n^{-1}\mathbf{\Sigma}_k) - \frac{\tau_n}{2}(\mathbf{x}_n - \mathbf{\mu}_k)^\top \mathbf{\Sigma}_k^{-1}(\mathbf{x}_n - \mathbf{\mu}_k) \bigg).\label{eq:2022 Q5 log like}
    \end{align}
    On the exam, we are only asked to do the M-step for $\{\mathbf{\mu}_k,\mathbf{\Sigma}_k\}_{k=1}^K$. We can therefore drop terms that do not depend on $\mathbf{\mu}_k$ or $\mathbf{\Sigma}_k$. This gives,
    \begin{align*}
        &\log p(z_n,\tau_n,\mathbf{z}_n;\mathbf{\theta})\\
        &=\sum_{k=1}^K I[z_n = k]\bigg(-\frac{1}{2}\log \det(\tau_n^{-1}\mathbf{\Sigma}_k) - \frac{\tau_n}{2}(\mathbf{x}_n - \mathbf{\mu}_k)^\top \mathbf{\Sigma}_k^{-1}(\mathbf{x}_n - \mathbf{\mu}_k) \bigg)\\
        &=\sum_{k=1}^K I[z_n = k]\bigg(\frac{D}{2}\log\tau_n-\frac{1}{2}\log \det(\mathbf{\Sigma}_k) - \frac{\tau_n}{2}(\mathbf{x}_n - \mathbf{\mu}_k)^\top \mathbf{\Sigma}_k^{-1}(\mathbf{x}_n - \mathbf{\mu}_k) \bigg)\\
        &=\sum_{k=1}^K I[z_n = k]\bigg(-\frac{1}{2}\log \det(\mathbf{\Sigma}_k) - \frac{\tau_n}{2}(\mathbf{x}_n - \mathbf{\mu}_k)^\top \mathbf{\Sigma}_k^{-1}(\mathbf{x}_n - \mathbf{\mu}_k) \bigg),
    \end{align*}
    where in the last line we dropped the term $\frac{D}{2}\log\tau_n$. We will now take an expectation with respect to $p(z_n,\tau_n\mid \mathbf{x}_n ;\mathbf{\theta}_{\mathrm{old}})$. By parts (b) and (c) we have,
    \begin{align*}
        \bbE_{p(z_n, \tau_n \mid \mathbf{x}_n;\mathbf{\theta}_{\mathrm{old}})}\left[I[Z_n = k]\right] = \omega_{nk}^{\mathrm{old}},
    \end{align*}
    and 
    \begin{align*}
    \bbE_{p(z_n,\tau_n \mid \mathbf{x}_n;\mathbf{\theta}_{\mathrm{old}})}\left[I[Z_n=k]\tau_n\right]
        &=\omega_{nk}^{\mathrm{old}}\bbE_{\mathrm{Ga}(z_n;\alpha_{nk}^{\text{old}}, \beta_{nk}^{old})}\left[\tau_n\right]\\
        &= \frac{\omega_{nk}^{\mathrm{old}}\alpha_{nk}^{\text{old}}}{\beta_{nk}^{\text{old}}}.
    \end{align*}
    Thus, up to a constant independent of $\{\mathbf{\mu}_k,\mathbf{\Sigma}_k\}_{k=1}^K$ we have
    \begin{align*}
        &\bbE_{p(z_n, \tau_n \mid \mathbf{x}_n;\mathbf{\theta}_{\mathrm{old}})}\left[\log p(z_n,\tau_n,\mathbf{z}_n;\mathbf{\theta})\right]\\
        &=\sum_{k=1}^K \bbE_{p(z_n, \tau_n \mid \mathbf{x}_n;\mathbf{\theta}_{\mathrm{old}})}\left[I[z_n = k]\bigg(-\frac{1}{2}\log \det(\mathbf{\Sigma}_k) - \frac{\tau_n}{2}(\mathbf{x}_n - \mathbf{\mu}_k)^\top \mathbf{\Sigma}_k^{-1}(\mathbf{x}_n - \mathbf{\mu}_k) \bigg)\right]\\
        &=\sum_{k=1}^K \omega_{nk}^{\mathrm{old}}\left(-\frac{1}{2}\log \det(\mathbf{\Sigma}_k) - \frac{\alpha_{nk}^{\mathrm{old}}}{2\beta_{nk}^{\mathrm{old}}}(\mathbf{x}_n - \mathbf{\mu}_k)^\top \mathbf{\Sigma}_k^{-1}(\mathbf{x}_n - \mathbf{\mu}_k) \right)
    \end{align*}
    And so, up to terms independent of $\{\mathbf{\mu}_k,\mathbf{\Sigma}_k\}_{k=1}^K$, we have
    \begin{align*}
        &\mathcal{L}(\mathbf{\theta};\mathbf{\theta}_{\mathrm{old}})\\
        &=\sum_{n=1}^N\sum_{k=1}^K  \omega_{nk}^{\mathrm{old}}\left(-\frac{1}{2}\log \det(\mathbf{\Sigma}_k) - \frac{\alpha_{nk}^{\mathrm{old}}}{2\beta_{nk}^{\mathrm{old}}}(\mathbf{x}_n - \mathbf{\mu}_k)^\top \mathbf{\Sigma}_k^{-1}(\mathbf{x}_n - \mathbf{\mu}_k) \right)\\
        &=\sum_{k=1}^K\sum_{n=1}^N \omega_{nk}^{\mathrm{old}}\left(-\frac{1}{2}\log \det(\mathbf{\Sigma}_k) - \frac{\alpha_{nk}^{\mathrm{old}}}{2\beta_{nk}^{\mathrm{old}}}(\mathbf{x}_n - \mathbf{\mu}_k)^\top \mathbf{\Sigma}_k^{-1}(\mathbf{x}_n - \mathbf{\mu}_k) \right).
    \end{align*}
    We see that the M-step for $\{\mathbf{\mu}_k,\mathbf{\Sigma}_k\}_{k=1}^K$ splits over $k$. And so,
    \[\mathbf{\mu}_k^\star,\mathbf{\Sigma}_k^\star \gets \argmax_{\mathbf{\mu}_k,\mathbf{\Sigma}_k} \sum_{n=1}^N \omega_{nk}^{\mathrm{old}}\left(-\frac{1}{2}\log \det(\mathbf{\Sigma}_k) - \frac{\alpha_{nk}^{\mathrm{old}}}{2\beta_{nk}^{\mathrm{old}}}(\mathbf{x}_n - \mathbf{\mu}_k)^\top \mathbf{\Sigma}_k^{-1}(\mathbf{x}_n - \mathbf{\mu}_k) \right). \]
    If we differentiate with respect to $\mathbf{\mu}_k$, we see that $\mathbf{\mu}_k^\star$ solves,
    \[\sum_{n=1}^N \frac{\omega_{nk}^{\mathrm{old}}\alpha_{nk}^{\mathrm{old}}}{\beta_{nk}^{\mathrm{old}}}\mathbf{\Sigma}^{-1} (\mathbf{x}_n-\mathbf{\mu}_k^\star)=0\in \reals^D. \]
    And so,
    \begin{align*}
       \mathbf{\mu}_k^\star \left(\sum_{n=1}^N \frac{\omega_{nk}^{\mathrm{old}}\alpha_{nk}^{\mathrm{old}}}{\beta_{nk}^{\mathrm{old}}}\right) &= \sum_{n=1}^N \frac{\omega_{nk}^{\mathrm{old}}\alpha_{nk}^{\mathrm{old}}}{\beta_{nk}^{\mathrm{old}}} \mathbf{x}_n.
    \end{align*}
    If we define 
    \[\gamma_{nk} = \frac{\omega_{nk}^{\mathrm{old}}\alpha_{nk}^{\mathrm{old}}/\beta_{nk}^{\mathrm{old}}}{\sum_{m=1}^N \omega_{mk}^{\mathrm{old}}\alpha_{mk}^{\mathrm{old}}/\beta_{mk}^{\mathrm{old}}}, \]
    then 
    \[ \mathbf{\mu}^\star = \sum_{n=1}^N \gamma_{nk} \mathbf{x}_n.\]
    Likewise, plugging in $\mathbf{\mu}^\star$ and differentiating with respect to $\mathbf{\Sigma}_k^{-1}$ gives the first order condition,
    \[\frac{1}{2}\sum_{n=1}^N \omega_{nk}^{\text{old}}\mathbf{\Sigma}_k^\star - \frac{\omega_{nk}^{\mathrm{old}}\alpha_{nk}^{\mathrm{old}}}{\beta_{nk}^{\mathrm{old}}}(\mathbf{x}_n-\mathbf{\mu}_k^\star)(\mathbf{x}_n - \mathbf{\mu}_k^\star)^\top = 0 \in \reals^{D \times D}. \]
    And so
    \[\mathbf{\Sigma}_k^\star = \frac{1}{\sum_{n=1}^N \omega_{nk}^\text{old}} \sum_{n=1}^N \frac{\omega_{nk}^{\mathrm{old}}\alpha_{nk}^{\mathrm{old}}}{\beta_{nk}^{\mathrm{old}}}(\mathbf{x}_n-\mathbf{\mu}_k^\star)(\mathbf{x}_n - \mathbf{\mu}_k^\star)^\top.  \]
\end{enumerate}
\textbf{Optional:}  In terms of exam requirements, we are now done. We have calculated the M-step for $\{(\mathbf{\mu}_k,\mathbf{\Sigma}_k)\}_{k=1}^K$ which we did by isolating the part of $\mathcal{L}(\mathbf{\theta};\mathbf{\theta}^{\text{old}})$ that depended on these parameters. We can do a similar thing for $\mathbf{\pi}$. Using \eqref{eq:2022 Q5 log like} we can compute $\log p(z_n,\tau_n,\mathbf{x}_n;\mathbf{\theta})$ as a function of only $\mathbf{\pi}$ specifically,
\[\log p(z_n,\tau_n,\mathbf{x}_n;\mathbf{\theta}) = C +\sum_{k=1}^K I[Z_n = k]\log(\pi_k). \]
Thus, as a function of $\mathbf{\pi}$,
\begin{align*}
    &\mathcal{L}(\mathbf{\theta};\mathbf{\theta}^{\text{old}})\\
    &=\sum_{n=1}^N\bbE_{p(z_n,\tau_n\mid \mathbf{x}_n;\mathbf{\theta}^{\text{old}})}\left[\sum_{k=1}^KI[Z_n=k]\log(\pi_k)\right]\\
    &=\sum_{n=1}^N \sum_{k=1}^K\omega_{nk}^{\text{old}}\log(\pi_k).
\end{align*}
Thus,
\[\pi_k^\star = \frac{\sum_{n=1}^N \omega_{nk}}{\sum_{j=1}^K\sum_{n=1}^N \omega_{nj}}. \]
We can be daring and attempt the $\nu_k$ updates as well. Up to a constant that does not depend on $\nu_k$ we have
\begin{align*}
   & p(z_n,\tau_n,\mathbf{x}_n;\mathbf{\theta})\\
   &=C+\sum_{k=1}^KI[z_n = k]\left(\frac{\nu_k}{2}\log(\nu_k/2)-\log \Gamma(\nu_k/2) + \nu_k/2\log(\tau_n) -\frac{\nu_k}{2}\tau_n\right).
\end{align*}
Let $v_k = \nu_k/2$. Thus, as a function of $v_k$ we have
\begin{align*}
    & p(z_n,\tau_n,\mathbf{x}_n;\mathbf{\theta})\\
   &=C+\sum_{k=1}^KI[z_n = k]\left(v_k\log(v_k)-\log \Gamma(v_k) + v_k\log(\tau_n) -v_k\tau_n\right).
\end{align*}
We also have
\begin{align*}
    \bbE_{p(z_n,\tau_n\mid \mathbf{x}_n;\mathbf{\theta}^{\text{old}})}\left[I[Z_n=k]\right]&=\omega_{nk}^{\mathrm{old}},\\
    \bbE_{p(z_n,\tau_n\mid \mathbf{x}_n;\mathbf{\theta}^{\text{old}})}\left[I[Z_n=k]\tau_n\right]&=\omega_{nk}^{\mathrm{old}}\alpha_{nk}^{\text{old}}/\beta_{nk}^{\text{old}},\\
    \bbE_{p(z_n,\tau_n\mid \mathbf{x}_n;\mathbf{\theta}^{\text{old}})}\left[I[Z_n=k]\log(\tau_n)\right]&=\omega_{nk}^{\mathrm{old}}\bbE_{\mathrm{Ga}(\tau_n;\alpha_{nk}^{\text{old}},\beta_{nk}^{\text{old}})}\left[\log(\tau_n)\right]\\
    &=\omega_{nk}^{\mathrm{old}}\left(\psi\left(\alpha_{nk}^{\text{old}}\right) - \log\left(\beta_{nk}^{\text{old}}\right)\right),
\end{align*}
where
\[\psi(y)= \frac{d}{dy}\log \Gamma(y), \]
is the digamma function. Thus, as a function of $\mathbf{v}=(v_k)_{k=1}^K$, we have
\begin{align*}
    &\mathcal{L}(\mathbf{\theta};\mathbf{\theta}_{\text{old}})\\
    &=\sum_{n=1}^N \sum_{k=1}^K \omega_{nk}^{\text{old}}\left(v_k\log(v_k)-\log \Gamma(v_k)+v_k\left(\psi\left(\alpha_{nk}^{\text{old}}\right) - \log\left(\beta_{nk}^{\text{old}}\right)\right) -v_k\alpha_{nk}^{\text{old}}/\beta_{nk}^{\text{old}}\right).
\end{align*}
And so $v_k^\star$ maximizes
\[ \left(v_k\log(v_k)-\log \Gamma(v_k)\right)\sum_{n=1}^N\omega_{nk} +v_k \sum_{n=1}^N \omega_{nk}^{\text{old}}\left(\psi\left(\alpha_{nk}^{\text{old}}\right) - \log\left(\beta_{nk}^{\text{old}}\right)-\alpha_{nk}^{\text{old}}/\beta_{nk}^{\text{old}}\right).  \]
Define,
\begin{align*}
    A_k &= \sum_{n=1}^N\omega_{nk},\\
    B_k &= \sum_{n=1}^N \omega_{nk}^{\text{old}}\left(\psi\left(\alpha_{nk}^{\text{old}}\right) - \log\left(\beta_{nk}^{\text{old}}\right)-\alpha_{nk}^{\text{old}}/\beta_{nk}^{\text{old}}\right).
\end{align*}
We are trying to maximize
\[(v_k\log(v_k)-\log \Gamma(v_k))A_k + v_kB_k. \]
Which has the first order condition
\[\left(\log(v_k^\star)+1 -\psi(v_k^\star)\right)A_k + B_k = 0 \]
So
\[\log(v_k^\star) - \psi(v_k^\star) = -\frac{A_k+B_k}{A_k}. \]
A plot in Mathematic shows that $v \mapsto \log(v) - \psi(v)$ is strictly decreasing and so the above equation has a unique solution $v_k^\star$. This means that, with a bit of work, we can implement a full EM algorithm for all the parameters $\mathbf{\theta}$. An imperfect implementation is available here \url{https://github.com/Michael-Howes/Applied-Coaching/blob/main/Code/student_mixture.ipynb}.

\subsection*{Question 6: Principal component regression with $\ell_1$-penalties}

Key ideas:
\begin{itemize}
    \item The role of the SVD in least squares regression and ridge regression.
    \item Reducing a multi-parameter optimization problem into several one0dimensional problems.
    \item $\ell_1$-penalties inducing sparsity.
\end{itemize}

\begin{enumerate}[label = (\alph*)]
    \item The function,
    \[S_\lambda(t) := \argmin_{s \in \reals} \left\{\frac{1}{2}(s-t)^2 + \lambda |s|\right\}, \]
    is the \emph{soft-threshold} function. It is given by,
    \[S_\lambda(t) = \begin{cases}
        t+\lambda &\text{if } t <-\lambda,\\
        0 & \text{if } -\lambda \le t \le \lambda, \\
        t-\lambda & \text{if } t  > \lambda.
    \end{cases} \] 
    In convex optimization it is called the \emph{proximal function} for the absolute value function $|\cdot|$. On the qualifying exam, you are not asked to derive the above expression for $S_\lambda(t)$, but we will go through a derivation here.

    Define $f_t(s) = \frac{1}{2}(s-t)^2 + \lambda |s|$. Note that,
    \[f_t(s) = \begin{cases}
        \frac{1}{2}(s-t)^2 - \lambda s & \text{if } s \le 0,\\
        \frac{1}{2}(s-t)^2 + \lambda s & \text{if } s > 0.
    \end{cases} \]
    The above function is convex as it is the maximum of two convex functions. We'll now consider different cases. 
    
    If $t < -\lambda$, then $s^\star := t+\lambda < 0$. For $s<0$, $f_t(s)$ is differentiable with derivate $s-t - \lambda$. Thus, $s^\star = t+\lambda<0$ solves the first order conditions and is a minimizer of $f_t(s)$.

    Likewise, if $t>\lambda$, the $s^\star := t-\lambda > 0$. For $s>0$, $f_t(s)$ is differentiable with derivate $s-t+\lambda$. Thus, $s^\star = t-\lambda >0$ is a minimizer of $f_t(s)$. 

    Finally, if $-\lambda \le t \le \lambda$, then the first order conditions have no stationary points and so the minimizer must be where $f_t(s)$ is non-differentiable. That is, $s^\star = 0$.
    \item We are asked to find,
    \[\widehat{\beta}_\lambda := \argmin_{b \in \reals^d}\left\{\frac{1}{2}\Vert Xb - y \Vert_2^2 + \sum_{j=1}^d \lambda_j |v_j^\top b |\right\}, \]
    where $V = [v_1, \ldots, v_d] \in \reals^{d \times d}$ are the right singular vectors of $X$. Specifically we are given,
    \[X = U\Gamma V^\top, \]
    where
    \begin{itemize}
        \item $U =[u_1,\ldots, u_d]\in \reals^{n \times d}$ with $U^\top U = I_{d}$.
        \item $\Gamma$ is a $d \times d$ diagonal matrix of singular values $\Gamma=\diag(\gamma_1,\ldots,\gamma_d)$ with $\gamma_1 \ge \cdots \ge \gamma_d > 0$.
        \item $V = [v_1, \ldots, v_d] \in \reals^{d \times d}$ satisfies $V^\top V =VV^\top = I_d$. 
    \end{itemize}
    For all $b \in \reals^d$, we have
    \begin{align*}
        Xb - Y &= U\Gamma V^\top b - Y\\
        &=U\Gamma V^\top b - UU^\top Y - (I-UU^\top)Y\\
        &=U(\Gamma V^\top b - U^\top Y) - (I-UU^\top)Y.
    \end{align*}
    Note that
    \[U^\top(I-UU^\top) =U^\top - U^\top = 0,  \]
    and
    \[(I-UU^\top)U = U-U=0.\]
    The vectors $U(\Gamma V^\top b-U^\top Y) \in \reals^n$ and $(I-UU^\top)Y \in \reals^n$ are therefor orthogonal to each other. Thus,
    \begin{align*}\Vert Xb - Y \Vert_2^2 = \Vert U(\Gamma V^\top b - U^\top Y) - (I-UU^\top)Y\Vert_2^2 = \Vert U(\Gamma V^\top b - U^\top Y)\Vert_2^2 + \Vert (I-UU^\top)Y\Vert_2^2.
    \end{align*}
    Since the second term does not depend on $b$, we can conclude that $\widehat{\beta}_\lambda$ solves
    \[\widehat{\beta}_\lambda = \argmin_{b \in \reals^n}\left\{\frac{1}{2}\Vert U (\Gamma V^\top b - U^\top Y)\Vert_2^2 + \sum_{j=1}^d \lambda_j |v_j^\top b|\right\}. \]
    Since $U^\top U = I_d$, we have that $\Vert Ux\Vert_2^2 = \Vert x \Vert_2^2$ for all $x \in \reals^d$. Thus,
    \begin{align*}
        \Vert U (\Gamma V^\top b - U^\top Y)\Vert_2^2 &= \Vert \Gamma V^\top b - U^\top Y \Vert_2^2.
    \end{align*}
    Let $c = V^\top b \in \reals^d$ so that $c_j = v_j^\top b$. Since $b = Vc$, we have that $\hat{\beta}_\lambda$ satisfies,
    \begin{align*}
        \widehat{\beta}_\lambda &= Vc^\star,\\
        \text{where } c^\star &= \argmin_{c \in \reals^d} \left\{\Vert \Gamma c - U^\top Y \Vert_2^2 + \sum_{j=1}^d \lambda_j |c_j| \right\}. 
    \end{align*}
    Since $\Gamma$ is diagonal we have that,
    \begin{align*}
        \Vert \Gamma c - U^\top Y \Vert_2^2 + \sum_{j=1}^d \lambda_j |c_j|&=\sum_{j=1}^d \left(\gamma_j c_j - u_j^\top Y\right)^2 + \sum_{j=1}^d \lambda_j |c_j|\\
        &=\sum_{j=1}^d \left(\gamma_j c_j - u_j^\top Y\right)^2 + \lambda_j |c_j|\\
        &=\sum_{j=1}^d \gamma_j^2\left[\left(c_j - \frac{u_j^\top Y}{\gamma_j}\right)^2 + \frac{\lambda_j}{\gamma_j^2} |c_j|\right].
    \end{align*}
    The objective function for $c$ is therefor separable over the coordinates $c_j$. We can thus calculate $c^\star$ by solving $p$ one-dimensional optimization problems. Specifically,
    \begin{align*}
        c_j^\star &=\argmin_{c_j} \left\{\gamma_j^2\left[\left(c_j - \frac{u_j^\top Y}{\gamma_j}\right)^2 + \frac{\lambda_j}{\gamma_j^2} |c_j|\right]\right\} \\
        &=\argmin_{c_j}\left\{\left(c_j - \frac{u_j^\top Y}{\gamma_j}\right)^2 + \frac{\lambda_j}{\gamma_j^2}  |c_j|\right\}\\
        &= S_{\frac{\lambda_j}{\gamma_j^2}}\left(\frac{u_j^\top Y}{\gamma_j}\right)
    \end{align*} 
    where $S_\lambda(t)$ is the soft-threshold function as in part (a). It follows that,
    \[\widehat{\beta}_\lambda =Vc^\star = \sum_{j=1}^d  S_{\frac{\lambda_j}{\gamma_j^2}}\left(\frac{u_j^\top Y}{\gamma_j}\right) v_j \]
    \item We now assume that $\gamma_j=1$ for all $j$. We know that $\widehat{\beta}_{\mathrm{ridge}}$ can also be described in terms of the SVD of $X$. Specifically, in the special case when $\gamma_j=1$ for all $j$
    \[\widehat{\beta}_{\mathrm{ridge}} = \sum_{j=1}^d \frac{1}{1+\lambda_{\mathrm{ridge}}} u_j^\top Y v_j. \] 
    And, from part (b),
    \[\widehat{\beta}_\lambda = \sum_{j=1}^d  S_{\lambda_j}\left(u_j^\top Y\right) v_j\]
    We see that when written in the basis $v_1,\ldots,v_d$, the ridge coefficients and principal-components $\ell_1$-regularized are both shrunk version of the OLS coefficients $a_j = u_j^\top Y$. Specifically, 
    \begin{itemize}
        \item Ridge coefficients: $\frac{1}{1+\lambda_{\mathrm{ridge}}} a_j$.
        \item $\ell_1$-regularized components: $S_{\lambda_j}(a_j)$.
    \end{itemize}
    The performance of these two estimators will depend on the expansion of the true parameter vector $\beta$ in the basis $v_1,\ldots,v_d$. If when we write $\beta = \sum_{j=1}^d b_jv_j$, many of the $b_j$'s are zero. Then we will expect the $\ell_1$-regularized estimator to perform better. This is because the soft threshold function $S_{\lambda_j}(a_j)$ will set some coefficients equal to zero. On the other the ridge shrinkage simply scales each $a_j$ by a constant amount. 
\end{enumerate}